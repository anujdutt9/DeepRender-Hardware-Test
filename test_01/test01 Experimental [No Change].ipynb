{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:scikit-learn version 0.23.1 is not supported. Minimum required version: 0.17. Maximum required version: 0.19.2. Disabling scikit-learn conversion API.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import coremltools as ct\n",
    "from coremltools.models.neural_network import quantization_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_path = \"./model_parameter.pt\"\n",
    "valid_dir = \"./kodak/\"\n",
    "y_hat_dir = \"./y_hat/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function to get Nearest Weights\n",
    "def get_nearestup_weight(cin):\n",
    "    filt=torch.Tensor([[0, 0, 0, 0], [0, 1, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]])[None, None, ...]\n",
    "    weight = np.zeros((cin, cin, 4, 4),\n",
    "                      dtype=np.float64)\n",
    "    weight[range(cin), range(cin), :, :] = filt\n",
    "    return torch.from_numpy(weight).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "\n",
    "        # Define Decoder convolution and activation layers\n",
    "        self.Conv_D_1 = torch.nn.Conv2d(in_channels=12,  out_channels=192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode='zeros', bias=True)\n",
    "        self.Conv_D_2 = torch.nn.Conv2d(in_channels=192, out_channels=192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode='zeros', bias=True)\n",
    "        self.Conv_D_3 = torch.nn.Conv2d(in_channels=192, out_channels=192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode='zeros', bias=True)\n",
    "        self.Conv_D_4 = torch.nn.Conv2d(in_channels=192, out_channels=3,   kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode='zeros', bias=True)\n",
    "\n",
    "        self.Act_D_1 = nn.PReLU(init=0.2)\n",
    "        self.Act_D_2 = nn.PReLU(init=0.2)\n",
    "        self.Act_D_3 = nn.PReLU(init=0.2)\n",
    "\n",
    "    def forward(self, y_hat):\n",
    "\n",
    "        # Decoder: y_hat --> x_hat\n",
    "        Conv_D_1 = self.Act_D_1(self.Conv_D_1(y_hat))\n",
    "        #Conv_D_2 = self.Act_D_2(self.Conv_D_2(F.interpolate(Conv_D_1, mode='nearest', scale_factor=2)))\n",
    "        #Conv_D_3 = self.Act_D_3(self.Conv_D_3(F.interpolate(Conv_D_2, mode='nearest', scale_factor=2)))\n",
    "        \n",
    "        # Since, CoreML/ONNX converter Tool does not support Upsample layer,\n",
    "        # replacing the layer with 2D Transposed Comvolution Layers\n",
    "        upweight_1 = get_nearestup_weight(Conv_D_1.size(1))\n",
    "        Conv_D_2 = self.Act_D_2(self.Conv_D_2(F.conv_transpose2d(Conv_D_1, upweight_1.detach(),stride=2, padding=1)))\n",
    "        \n",
    "        upweight_2 = get_nearestup_weight(Conv_D_2.size(1))\n",
    "        Conv_D_3 = self.Act_D_3(self.Conv_D_3(F.conv_transpose2d(Conv_D_2, upweight_2.detach(),stride=2, padding=1)))\n",
    "        \n",
    "        x_hat    = self.Conv_D_4(Conv_D_3)\n",
    "\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points to Remember\n",
    "\n",
    "**Ground Truth Image shape** = (1, 3, 512, 768)\n",
    "\n",
    "**y_hat** represents the test input Image\n",
    "\n",
    "**y_hat.shape** = (1, 12, 128, 192)\n",
    "\n",
    "***model takes y_hat as input and returns compressed image.***\n",
    "\n",
    "**Model Prediction Output Shape** = (1, 3, 512, 768)\n",
    "\n",
    "Then we calculate the MSE between the Original Image and the Model Predicted Image and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring weights from ./model_parameter.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EncoderDecoder().to(device)\n",
    "model.eval()\n",
    "\n",
    "# Restore Paths:\n",
    "print(f'Restoring weights from {restore_path}')\n",
    "checkpoint = torch.load(restore_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-bc49a379f27e>:3: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  filt=torch.Tensor([[0, 0, 0, 0], [0, 1, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]])[None, None, ...]\n",
      "<ipython-input-4-bc49a379f27e>:4: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  weight = np.zeros((cin, cin, 4, 4),\n",
      "<ipython-input-4-bc49a379f27e>:6: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  weight[range(cin), range(cin), :, :] = filt\n",
      "<ipython-input-4-bc49a379f27e>:7: TracerWarning: torch.from_numpy results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return torch.from_numpy(weight).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%y_hat : Float(1, 12, 128, 192),\n",
      "      %Conv_D_1.weight : Float(192, 12, 3, 3),\n",
      "      %Conv_D_1.bias : Float(192),\n",
      "      %Conv_D_2.weight : Float(192, 192, 3, 3),\n",
      "      %Conv_D_2.bias : Float(192),\n",
      "      %Conv_D_3.weight : Float(192, 192, 3, 3),\n",
      "      %Conv_D_3.bias : Float(192),\n",
      "      %Conv_D_4.weight : Float(3, 192, 3, 3),\n",
      "      %Conv_D_4.bias : Float(3),\n",
      "      %26 : Float(1, 1, 1),\n",
      "      %27 : Float(1, 1, 1),\n",
      "      %28 : Float(1, 1, 1)):\n",
      "  %12 : Float(1, 192, 128, 192) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%y_hat, %Conv_D_1.weight, %Conv_D_1.bias) # /Users/anujdutt/miniconda3/envs/PyTorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:345:0\n",
      "  %14 : Float(1, 192, 128, 192) = onnx::PRelu(%12, %26) # /Users/anujdutt/miniconda3/envs/PyTorch/lib/python3.8/site-packages/torch/nn/functional.py:1263:0\n",
      "  %15 : Float(192, 192, 4, 4) = onnx::Constant[value=<Tensor>]()\n",
      "  %16 : Float(1, 192, 256, 384) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[4, 4], pads=[1, 1, 1, 1], strides=[2, 2]](%14, %15) # <ipython-input-5-7fef3c0cd7b4>:25:0\n",
      "  %17 : Float(1, 192, 256, 384) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%16, %Conv_D_2.weight, %Conv_D_2.bias) # /Users/anujdutt/miniconda3/envs/PyTorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:345:0\n",
      "  %19 : Float(1, 192, 256, 384) = onnx::PRelu(%17, %27) # /Users/anujdutt/miniconda3/envs/PyTorch/lib/python3.8/site-packages/torch/nn/functional.py:1263:0\n",
      "  %20 : Float(192, 192, 4, 4) = onnx::Constant[value=<Tensor>]()\n",
      "  %21 : Float(1, 192, 512, 768) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[4, 4], pads=[1, 1, 1, 1], strides=[2, 2]](%19, %20) # <ipython-input-5-7fef3c0cd7b4>:28:0\n",
      "  %22 : Float(1, 192, 512, 768) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%21, %Conv_D_3.weight, %Conv_D_3.bias) # /Users/anujdutt/miniconda3/envs/PyTorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:345:0\n",
      "  %24 : Float(1, 192, 512, 768) = onnx::PRelu(%22, %28) # /Users/anujdutt/miniconda3/envs/PyTorch/lib/python3.8/site-packages/torch/nn/functional.py:1263:0\n",
      "  %pred : Float(1, 3, 512, 768) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%24, %Conv_D_4.weight, %Conv_D_4.bias) # /Users/anujdutt/miniconda3/envs/PyTorch/lib/python3.8/site-packages/torch/nn/modules/conv.py:345:0\n",
      "  return (%pred)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dummy input\n",
    "dummy_input = torch.rand(1, 12, 128, 192)\n",
    "\n",
    "# Define input / output names\n",
    "input_names = [\"y_hat\"]\n",
    "output_names = [\"pred\"]\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                  dummy_input,\n",
    "                  \"test01_network_op11.onnx\",\n",
    "                  verbose=True,\n",
    "                  input_names=input_names,\n",
    "                  output_names=output_names,\n",
    "                  opset_version= 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/9: Converting Node Type Conv\n",
      "2/9: Converting Node Type PRelu\n",
      "3/9: Converting Node Type ConvTranspose\n",
      "4/9: Converting Node Type Conv\n",
      "5/9: Converting Node Type PRelu\n",
      "6/9: Converting Node Type ConvTranspose\n",
      "7/9: Converting Node Type Conv\n",
      "8/9: Converting Node Type PRelu\n",
      "9/9: Converting Node Type Conv\n",
      "Translation to CoreML spec completed. Now compiling the CoreML model.\n",
      "Model Compilation done.\n"
     ]
    }
   ],
   "source": [
    "# Convert from ONNX to Core ML\n",
    "coreml_model  = ct.converters.onnx.convert(model='test01_network_op11.onnx', minimum_ios_deployment_target='13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model.input_description['y_hat'] = 'Input Image'\n",
    "coreml_model.output_description['pred'] = 'Compressed Image Output'\n",
    "coreml_model.short_description = 'FP-32 model.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input {\n",
       "  name: \"y_hat\"\n",
       "  shortDescription: \"Input Image\"\n",
       "  type {\n",
       "    multiArrayType {\n",
       "      shape: 1\n",
       "      shape: 12\n",
       "      shape: 128\n",
       "      shape: 192\n",
       "      dataType: FLOAT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "output {\n",
       "  name: \"pred\"\n",
       "  shortDescription: \"Compressed Image Output\"\n",
       "  type {\n",
       "    multiArrayType {\n",
       "      shape: 1\n",
       "      shape: 3\n",
       "      shape: 512\n",
       "      shape: 768\n",
       "      dataType: FLOAT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "metadata {\n",
       "  shortDescription: \"FP-32 model.\"\n",
       "  userDefined {\n",
       "    key: \"com.github.apple.coremltools.source\"\n",
       "    value: \"onnx==1.7.0\"\n",
       "  }\n",
       "  userDefined {\n",
       "    key: \"com.github.apple.coremltools.version\"\n",
       "    value: \"4.0b1\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coreml_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model FP 32\n",
    "coreml_model.save('./test01_model.mlmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Quantization\n",
    "\n",
    "Quantize to: 16 and 8 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Quantize and Save the Models\n",
    "def quantize_model(base_mode=None, quantization_bits=None):\n",
    "    quantized_model = quantization_utils.quantize_weights(base_mode, nbits=quantization_bits)\n",
    "    save_path = './test01_model_' + str(quantization_bits) + 'bit.mlmodel'\n",
    "    quantized_model.short_description = str(quantization_bits) + ' bit quantized model.'\n",
    "    quantized_model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing using linear quantization\n",
      "Quantizing layer Conv_0\n",
      "Quantizing layer ConvTranspose_3\n",
      "Quantizing layer Conv_4\n",
      "Quantizing layer ConvTranspose_7\n",
      "Quantizing layer Conv_8\n",
      "Quantizing layer Conv_10\n",
      "Quantizing using linear quantization\n",
      "Optimizing Neural Network before Quantization:\n",
      "Finished optimizing network. Quantizing neural network..\n",
      "Quantizing layer Conv_0\n",
      "Quantizing layer ConvTranspose_3\n",
      "Quantizing layer Conv_4\n",
      "Quantizing layer ConvTranspose_7\n",
      "Quantizing layer Conv_8\n",
      "Quantizing layer Conv_10\n",
      "Quantizing using linear quantization\n",
      "Optimizing Neural Network before Quantization:\n",
      "Finished optimizing network. Quantizing neural network..\n",
      "Quantizing layer Conv_0\n",
      "Quantizing layer ConvTranspose_3\n",
      "Quantizing layer Conv_4\n",
      "Quantizing layer ConvTranspose_7\n",
      "Quantizing layer Conv_8\n",
      "Quantizing layer Conv_10\n",
      "Quantizing using linear quantization\n",
      "Optimizing Neural Network before Quantization:\n",
      "Finished optimizing network. Quantizing neural network..\n",
      "Quantizing layer Conv_0\n",
      "Quantizing layer ConvTranspose_3\n",
      "Quantizing layer Conv_4\n",
      "Quantizing layer ConvTranspose_7\n",
      "Quantizing layer Conv_8\n",
      "Quantizing layer Conv_10\n",
      "Quantizing using linear quantization\n",
      "Optimizing Neural Network before Quantization:\n",
      "Finished optimizing network. Quantizing neural network..\n",
      "Quantizing layer Conv_0\n",
      "Quantizing layer ConvTranspose_3\n",
      "Quantizing layer Conv_4\n",
      "Quantizing layer ConvTranspose_7\n",
      "Quantizing layer Conv_8\n",
      "Quantizing layer Conv_10\n"
     ]
    }
   ],
   "source": [
    "# Quantize the Model\n",
    "quant_bits = [16, 8, 4, 2, 1]\n",
    "\n",
    "for bit in quant_bits:\n",
    "    quantize_model(coreml_model, bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreML Model Testing\n",
    "\n",
    "## CoreML Model Testing\n",
    "\n",
    "1. Load the CoreML Model\n",
    "2. Loop through the data and test for MSE\n",
    "\n",
    "**GT image shape:**  (1, 3, 512, 768)\n",
    "\n",
    "**y_hat shape:**  torch.Size([1, 12, 512, 768])\n",
    "\n",
    "**Model prediction shape:**  (1, 3, 512, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_coreml_model(quantization=32):\n",
    "    if quantization == 32:\n",
    "        ml_model = ct.models.MLModel('./test01_model.mlmodel')\n",
    "    elif quantization == 16:\n",
    "        ml_model = ct.models.MLModel('./test01_model_16bit.mlmodel')\n",
    "    elif quantization == 8:\n",
    "        ml_model = ct.models.MLModel('./test01_model_8bit.mlmodel')\n",
    "    elif quantization == 4:\n",
    "        ml_model = ct.models.MLModel('./test01_model_4bit.mlmodel')\n",
    "    elif quantization == 2:\n",
    "        ml_model = ct.models.MLModel('./test01_model_2bit.mlmodel')\n",
    "    elif quantization == 1:\n",
    "        ml_model = ct.models.MLModel('./test01_model_1bit.mlmodel')\n",
    "    \n",
    "    print(ml_model)\n",
    "\n",
    "    tmp = []\n",
    "    \n",
    "    filelist_valid = np.sort([file for file in os.listdir(valid_dir) if file.endswith('.png')])\n",
    "\n",
    "    for j in range(0, len(filelist_valid)):\n",
    "        image = cv2.imread(valid_dir + filelist_valid[j]).astype(np.float32) / 255.0\n",
    "        image = np.expand_dims(np.transpose(image, [2,0,1]), axis=0)\n",
    "        print(\"GT image shape: \", image.shape)\n",
    "\n",
    "        y_hat = np.load(y_hat_dir + filelist_valid[j][:-4] + \".npy\")\n",
    "        print(\"y_hat shape: \", y_hat.shape)\n",
    "\n",
    "        pred = ml_model.predict({'y_hat': y_hat})\n",
    "\n",
    "        model_prediction = np.asarray(pred['pred'])\n",
    "        print(\"Model prediction shape: \", model_prediction.shape)\n",
    "\n",
    "        mse = np.mean((image - model_prediction) ** 2) * 255.0 ** 2\n",
    "        print(f\"Image: {filelist_valid[j]}, MSE: {mse}\")\n",
    "        tmp.append(mse)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "    \n",
    "    print(\"Model Quantization: \", quantization)\n",
    "    print(\"MSE Values: \",tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input {\n",
      "  name: \"y_hat\"\n",
      "  shortDescription: \"Input Image\"\n",
      "  type {\n",
      "    multiArrayType {\n",
      "      shape: 1\n",
      "      shape: 12\n",
      "      shape: 128\n",
      "      shape: 192\n",
      "      dataType: FLOAT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "output {\n",
      "  name: \"pred\"\n",
      "  shortDescription: \"Compressed Image Output\"\n",
      "  type {\n",
      "    multiArrayType {\n",
      "      shape: 1\n",
      "      shape: 3\n",
      "      shape: 512\n",
      "      shape: 768\n",
      "      dataType: FLOAT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata {\n",
      "  shortDescription: \"2 bit quantized model.\"\n",
      "  userDefined {\n",
      "    key: \"com.github.apple.coremltools.source\"\n",
      "    value: \"onnx==1.7.0\"\n",
      "  }\n",
      "  userDefined {\n",
      "    key: \"com.github.apple.coremltools.version\"\n",
      "    value: \"4.0b1\"\n",
      "  }\n",
      "}\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 0.png, MSE: 67499445093.75\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 1.png, MSE: 59796343814.0625\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 10.png, MSE: 50670011910.9375\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 11.png, MSE: 73630961212.5\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 12.png, MSE: 68218897950.0\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 13.png, MSE: 100246961700.0\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 14.png, MSE: 188035702368.75\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 15.png, MSE: 57124706343.75\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 16.png, MSE: 71106251793.75\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 17.png, MSE: 44053470253.125\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 18.png, MSE: 36886138396.875\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 19.png, MSE: 80064323381.25\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 2.png, MSE: 75817117968.75\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 20.png, MSE: 93728294859.375\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 21.png, MSE: 76238106075.0\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 22.png, MSE: 43329274635.9375\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 23.png, MSE: 35968529981.25\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 3.png, MSE: 92380537940.625\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 4.png, MSE: 56015010014.0625\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 5.png, MSE: 56244324740.625\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 6.png, MSE: 104274593943.75\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 7.png, MSE: 49300626051.5625\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 8.png, MSE: 123286188909.375\n",
      "\n",
      "\n",
      "\n",
      "GT image shape:  (1, 3, 512, 768)\n",
      "y_hat shape:  (1, 12, 128, 192)\n",
      "Model prediction shape:  (1, 3, 512, 768)\n",
      "Image: 9.png, MSE: 46194999539.0625\n",
      "\n",
      "\n",
      "\n",
      "Model Quantization:  2\n",
      "MSE Values:  [67499445093.75, 59796343814.0625, 50670011910.9375, 73630961212.5, 68218897950.0, 100246961700.0, 188035702368.75, 57124706343.75, 71106251793.75, 44053470253.125, 36886138396.875, 80064323381.25, 75817117968.75, 93728294859.375, 76238106075.0, 43329274635.9375, 35968529981.25, 92380537940.625, 56015010014.0625, 56244324740.625, 104274593943.75, 49300626051.5625, 123286188909.375, 46194999539.0625]\n"
     ]
    }
   ],
   "source": [
    "# Test all Models\n",
    "test_coreml_model(quantization = 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
